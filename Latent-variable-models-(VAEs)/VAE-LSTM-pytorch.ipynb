{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17442718",
   "metadata": {},
   "source": [
    "# VAE-LSTM [PyTorch]\n",
    "\n",
    "This notebook is based on the codes below\n",
    "\n",
    "https://github.com/Khamies/LSTM-Variational-AutoEncoder/tree/main\n",
    "\n",
    "https://github.com/Khamies/LSTM-Variational-AutoEncoder/blob/main/model.py\n",
    "\n",
    "We use the trained model to perform inferences (no training is done)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0d9994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afca5fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d303b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_VAE(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, latent_size, num_layers=1):\n",
    "        super(LSTM_VAE, self).__init__()\n",
    "    \n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        # Variables\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm_factor = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "        self.dictionary = PTB(data_dir=\"./data\", split=\"train\", \n",
    "                              create_data= False, max_sequence_length= 60)\n",
    "\n",
    "        # X: bsz * seq_len * vocab_size \n",
    "        # Embedding\n",
    "        self.embed = torch.nn.Embedding(num_embeddings= self.vocab_size,\n",
    "                                        embedding_dim= self.embed_size)\n",
    "\n",
    "        # Encoder Part: define the layers\n",
    "        self.encoder_lstm = torch.nn.LSTM(input_size= self.embed_size,\n",
    "                                          hidden_size= self.hidden_size,\n",
    "                                          batch_first=True, num_layers= self.num_layers)\n",
    "        self.mean = torch.nn.Linear(in_features= self.hidden_size * self.lstm_factor,\n",
    "                                    out_features= self.latent_size)\n",
    "        self.log_variance = torch.nn.Linear(in_features= self.hidden_size * self.lstm_factor, \n",
    "                                            out_features= self.latent_size)\n",
    "\n",
    "        # Decoder Part: define the layers                                       \n",
    "        self.init_hidden_decoder = torch.nn.Linear(in_features= self.latent_size,\n",
    "                                                   out_features= self.hidden_size * self.lstm_factor)\n",
    "        self.decoder_lstm = torch.nn.LSTM(input_size= self.embed_size, hidden_size= self.hidden_size, \n",
    "                                          batch_first = True, num_layers = self.num_layers)\n",
    "        self.output = torch.nn.Linear(in_features= self.hidden_size * self.lstm_factor, \n",
    "                                      out_features= self.vocab_size)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=2)\n",
    "    \n",
    "    #initialize the hidden cells [h,c] of lstm\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden_cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device)\n",
    "        state_cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(self.device)\n",
    "        return (hidden_cell, state_cell)\n",
    "    \n",
    "    #create the Embedding layer\n",
    "    def get_embedding(self, x):\n",
    "        x_embed = self.embed(x)\n",
    "        # Total length for pad_packed_sequence method = maximum sequence length\n",
    "        maximum_sequence_length = x_embed.size(1)\n",
    "        return x_embed, maximum_sequence_length\n",
    "    \n",
    "    def encoder(self, packed_x_embed,total_padding_length, hidden_encoder):\n",
    "        # pad the packed input.\n",
    "        packed_output_encoder, hidden_encoder = self.encoder_lstm(packed_x_embed, hidden_encoder)\n",
    "        output_encoder, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output_encoder, \n",
    "                                                                   batch_first=True, \n",
    "                                                                   total_length= total_padding_length)\n",
    "\n",
    "        # Extimate the mean and the variance of q(z|x)\n",
    "        mean = self.mean(hidden_encoder[0])\n",
    "        log_var = self.log_variance(hidden_encoder[0])\n",
    "        std = torch.exp(0.5 * log_var)   # e^(0.5 log_var) = var^0.5\n",
    "\n",
    "        # Generate a unit gaussian noise.\n",
    "        batch_size = output_encoder.size(0)\n",
    "        seq_len = output_encoder.size(1)\n",
    "        noise = torch.randn(batch_size, self.latent_size).to(self.device)\n",
    "\n",
    "        z = noise * std + mean\n",
    "\n",
    "        return z, mean, log_var, hidden_encoder\n",
    "    \n",
    "    def decoder(self, z, packed_x_embed, total_padding_length=None):\n",
    "        hidden_decoder = self.init_hidden_decoder(z)\n",
    "        hidden_decoder = (hidden_decoder, hidden_decoder)\n",
    "\n",
    "        # pad the packed input.\n",
    "        packed_output_decoder, hidden_decoder = self.decoder_lstm(packed_x_embed,hidden_decoder) \n",
    "        output_decoder, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output_decoder,\n",
    "                                                                   batch_first=True, \n",
    "                                                                   total_length= total_padding_length)\n",
    "\n",
    "        x_hat = self.output(output_decoder)\n",
    "        x_hat = self.log_softmax(x_hat)\n",
    "        return x_hat\n",
    "\n",
    "    def forward(self, x, sentences_length, hidden_encoder):  \n",
    "        \"\"\"\n",
    "            x : bsz * seq_len\n",
    "          hidden_encoder: ( num_lstm_layers * bsz * hidden_size, num_lstm_layers * bsz * hidden_size)\n",
    "        \"\"\"\n",
    "        # Get Embeddings\n",
    "        x_embed, maximum_padding_length = self.get_embedding(x)\n",
    "        # Packing the input\n",
    "        packed_x_embed = torch.nn.utils.rnn.pack_padded_sequence(input= x_embed, \n",
    "                                                                 lengths= sentences_length, \n",
    "                                                                 batch_first=True, enforce_sorted=False)\n",
    "        # Encoder\n",
    "        z, mean, log_var, hidden_encoder = self.encoder(packed_x_embed, maximum_padding_length, hidden_encoder)\n",
    "        # Decoder\n",
    "        x_hat = self.decoder(z, packed_x_embed, maximum_padding_length)\n",
    "        return x_hat, mean, log_var, z, hidden_encoder\n",
    "    \n",
    "    def inference(self, n_samples, sos, z):\n",
    "        # generate random z \n",
    "        batch_size = 1\n",
    "        seq_len = 1\n",
    "        idx_sample = []\n",
    "\n",
    "        input = torch.Tensor(1, 1).fill_(self.dictionary.get_w2i()[sos]).long().to(self.device)\n",
    "\n",
    "        hidden = self.init_hidden_decoder(z)\n",
    "        hidden = (hidden, hidden)\n",
    "\n",
    "        for i in range(n_samples):\n",
    "          input = self.embed(input)\n",
    "          output,hidden = self.decoder_lstm(input, hidden)\n",
    "          output = self.output(output)\n",
    "          output = self.log_softmax(output)\n",
    "          output = output.exp()\n",
    "          _, s = torch.topk(output, 1)\n",
    "          idx_sample.append(s.item())\n",
    "          input = s.squeeze(0)\n",
    "\n",
    "        w_sample = [self.dictionary.get_i2w()[str(idx)] for idx in idx_sample]\n",
    "        w_sample = \" \".join(w_sample)\n",
    "\n",
    "        return w_sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c036924f",
   "metadata": {},
   "source": [
    "# Define VAE Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbdc5530",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_Loss(torch.nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super(VAE_Loss, self).__init__()\n",
    "    self.nlloss = torch.nn.NLLLoss()\n",
    "  \n",
    "  def KL_loss (self, mu, log_var, z):\n",
    "    kl = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    kl = kl.sum(-1)  # to go from multi-dimensional z \n",
    "    #to single dimensional z : (batch_size x latent_size) ---> (batch_size) \n",
    "                            # i.e Z = [ [z1_1, z1_2 , ...., z1_lt] ] ------> z = [ z1] \n",
    "                            #         [ [z2_1, z2_2, ....., z2_lt] ]             [ z2]\n",
    "                            #                   .                                [ . ]\n",
    "                            #                   .                                [ . ]\n",
    "                            #         [[zn_1, zn_2, ....., zn_lt] ]              [ zn]\n",
    "                                                                      \n",
    "                            #        lt=latent_size \n",
    "    kl = kl.mean()                                                                    \n",
    "    return kl\n",
    "\n",
    "  def reconstruction_loss(self, x_hat_param, x):\n",
    "    x = x.view(-1).contiguous()\n",
    "    x_hat_param = x_hat_param.view(-1, x_hat_param.size(2))\n",
    "    recon = self.nlloss(x_hat_param, x)\n",
    "    return recon\n",
    "  \n",
    "\n",
    "  def forward(self, mu, log_var,z, x_hat_param, x):\n",
    "    kl_loss = self.KL_loss(mu, log_var, z)\n",
    "    recon_loss = self.reconstruction_loss(x_hat_param, x)\n",
    "    elbo = kl_loss + recon_loss # we use + because recon loss is a NLLoss (cross entropy) and it's negative in its own, and in the ELBO equation we have\n",
    "                              # elbo = KL_loss - recon_loss, therefore, ELBO = KL_loss - (NLLoss) = KL_loss + NLLoss\n",
    "    return elbo, kl_loss, recon_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15d8f96",
   "metadata": {},
   "source": [
    "# Define train loop\n",
    "\n",
    "https://github.com/Khamies/LSTM-Variational-AutoEncoder/blob/main/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17b4b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, train_loader, test_loader, model, loss, optimizer) -> None:\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.interval = 200\n",
    "\n",
    "\n",
    "    def train(self, train_losses, epoch, batch_size, clip) -> list:  \n",
    "        # Initialization of RNN hidden, and cell states.\n",
    "        states = self.model.init_hidden(batch_size) \n",
    "\n",
    "        for batch_num, batch in enumerate(self.train_loader): # loop over the data, and jump with step = bptt.\n",
    "            # get the labels\n",
    "            source, target, source_lengths = get_batch(batch)\n",
    "            source = source.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "            x_hat_param, mu, log_var, z, states = self.model(source,source_lengths, states)\n",
    "\n",
    "            # detach hidden states\n",
    "            states = states[0].detach(), states[1].detach()\n",
    "\n",
    "            # compute the loss\n",
    "            mloss, KL_loss, recon_loss = self.loss(mu = mu, log_var = log_var, z = z,\n",
    "                                                   x_hat_param = x_hat_param , x = target)\n",
    "            train_losses.append((mloss , KL_loss.item(), recon_loss.item()))\n",
    "            mloss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip)\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            if batch_num % self.interval == 0 and batch_num > 0:\n",
    "  \n",
    "                print('| epoch {:3d} | elbo_loss {:5.6f} | kl_loss {:5.6f} | recons_loss {:5.6f} '.format(\n",
    "                    epoch, mloss.item(), KL_loss.item(), recon_loss.item()))\n",
    "\n",
    "        return train_losses\n",
    "\n",
    "    def test(self, test_losses, epoch, batch_size) -> list:\n",
    "        with torch.no_grad():\n",
    "            states = self.model.init_hidden(batch_size) \n",
    "\n",
    "            for batch_num, batch in enumerate(self.test_loader): # loop over the data, and jump with step = bptt.\n",
    "                # get the labels\n",
    "                source, target, source_lengths = get_batch(batch)\n",
    "                source = source.to(self.device)\n",
    "                target = target.to(self.device)\n",
    "\n",
    "                x_hat_param, mu, log_var, z, states = self.model(source,source_lengths, states)\n",
    "\n",
    "                # detach hidden states\n",
    "                states = states[0].detach(), states[1].detach()\n",
    "\n",
    "                # compute the loss\n",
    "                mloss, KL_loss, recon_loss = self.loss(mu = mu, log_var = log_var,\n",
    "                                                       z = z, x_hat_param = x_hat_param , x = target)\n",
    "\n",
    "                test_losses.append((mloss , KL_loss.item(), recon_loss.item()))\n",
    "\n",
    "                # Statistics.\n",
    "                # if batch_num % 20 ==0:\n",
    "                #   print('| epoch {:3d} | elbo_loss {:5.6f} | kl_loss {:5.6f} | recons_loss {:5.6f} '.format(\n",
    "                #         epoch, mloss.item(), KL_loss.item(), recon_loss.item()))\n",
    "\n",
    "            return test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f100591b",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8473abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from data.ptb import PTB\n",
    "from settings import training_setting\n",
    "import torch\n",
    "\n",
    "def get_batch(batch):\n",
    "  sentences = batch[\"input\"]\n",
    "  target = batch[\"target\"]\n",
    "  sentences_length = batch[\"length\"]\n",
    "  return sentences, target, sentences_length\n",
    "\n",
    "def plot_elbo(losses, mode):\n",
    "    elbo_loss = list(map(lambda x: x[0], losses))\n",
    "    kl_loss = list(map(lambda x: x[1], losses))\n",
    "    recon_loss = list(map(lambda x: x[2], losses))\n",
    "\n",
    "    losses = {\"elbo\": elbo_loss, \"kl\": kl_loss, \"recon\": recon_loss}\n",
    "    print(losses)\n",
    "    for key in losses.keys():\n",
    "        plt.plot(losses.get(key), label=key+\"_\" + mode)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_latent_codes(dataloader, model, batch_size):\n",
    "  hidden = model.init_hidden(batch_size)\n",
    "  Z = []\n",
    "  with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "        x, t, leng = batch.get(\"input\"), batch.get(\n",
    "            \"target\"), batch.get(\"length\")\n",
    "        x = x.to(model.device)\n",
    "        t.to(model.device)\n",
    "        _, _, _, z, _ = model(x, leng, hidden)\n",
    "        Z.append(z)\n",
    "    Z = torch.cat(Z[:-1])\n",
    "    Z = Z.reshape(-1, Z.size(2))\n",
    "    return Z\n",
    "\n",
    "def visualize_latent_codes(z):\n",
    "    z = z.squeeze(0).t().contiguous()\n",
    "    n_z = z.size(0)\n",
    "    n = n_z//2\n",
    "    fig = plt.figure(figsize=(20, 6), dpi=80)\n",
    "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "    for i in range(1, 2*n):\n",
    "        ax = fig.add_subplot(2, n, i)\n",
    "        ax.hist(z[i].tolist())\n",
    "    plt.show()\n",
    "\n",
    "def interpolate(model, n_interpolations, sos, sequence_length):\n",
    "  z1 = torch.randn((1,1,model.latent_size)).to(model.device)\n",
    "  z2 = torch.randn((1,1,model.latent_size)).to(model.device)\n",
    "  text1 = model.inference(sequence_length , sos, z1)\n",
    "  text2 = model.inference(sequence_length , sos, z2)\n",
    "  alpha_s = torch.linspace(0,1,n_interpolations)\n",
    "  interpolations = torch.stack([alpha*z1 + (1-alpha)*z2  for alpha in alpha_s])\n",
    "  samples = [model.inference(sequence_length , sos, z) for z in interpolations]\n",
    "  return samples, text1, text2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c91e0d5",
   "metadata": {},
   "source": [
    "# Load data \n",
    "\n",
    "https://github.com/Khamies/LSTM-Variational-AutoEncoder/blob/main/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d71b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser.add_argument(\"--batch_size\", type=str, default=\"32\")\n",
    "#parser.add_argument(\"--bptt\", type=str,default=\"60\")\n",
    "#parser.add_argument(\"--embed_size\", type=str, default=\"300\") \n",
    "#parser.add_argument(\"--hidden_size\", type=str, default=\"256\")\n",
    "#parser.add_argument(\"--latent_size\", type=str, default=\"16\")\n",
    "#parser.add_argument(\"--lr\", type=str, default=\"0.001\")\n",
    "bptt = 60\n",
    "batch_size = 32\n",
    "embed_size = 300\n",
    "hidden_size = 256 #(LSTM hidden size)\n",
    "latent_size = 16 #(VAE latent size)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f42bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_data = PTB(data_dir=\"./data\", split=\"train\", create_data= False, max_sequence_length= bptt)\n",
    "test_data = PTB(data_dir=\"./data\", split=\"test\", create_data= False, max_sequence_length=bptt)\n",
    "valid_data = PTB(data_dir=\"./data\", split=\"valid\", create_data= False, max_sequence_length= bptt)\n",
    "\n",
    "# Batchify the data\n",
    "train_loader = torch.utils.data.DataLoader( dataset= train_data, batch_size=batch_size, shuffle= True)\n",
    "test_loader = torch.utils.data.DataLoader( dataset= test_data, batch_size= batch_size, shuffle= True)\n",
    "valid_loader = torch.utils.data.DataLoader( dataset= valid_data, batch_size= batch_size, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edb5d7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9839"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = train_data.vocab_size\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aaf68f",
   "metadata": {},
   "source": [
    "# Check data stored in loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f12de6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1315, 118, 106)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader), len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88053391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  {'input': tensor([[   2,   87, 7345,  ...,    0,    0,    0],\n",
       "           [   2,   35,   19,  ...,    0,    0,    0],\n",
       "           [   2,   10, 3536,  ...,    0,    0,    0],\n",
       "           ...,\n",
       "           [   2,  438, 3199,  ...,    0,    0,    0],\n",
       "           [   2,  224,  751,  ...,    0,    0,    0],\n",
       "           [   2,   87,  664,  ...,    0,    0,    0]]),\n",
       "   'target': tensor([[  87, 7345,   13,  ...,    0,    0,    0],\n",
       "           [  35,   19,   10,  ...,    0,    0,    0],\n",
       "           [  10, 3536,   79,  ...,    0,    0,    0],\n",
       "           ...,\n",
       "           [ 438, 3199,   77,  ...,    0,    0,    0],\n",
       "           [ 224,  751,  817,  ...,    0,    0,    0],\n",
       "           [  87,  664,    5,  ...,    0,    0,    0]]),\n",
       "   'length': tensor([36, 24,  7, 21, 19, 28, 29, 24, 10,  4, 33, 37,  8, 24, 44, 30, 45, 30,\n",
       "           14, 26, 13, 31, 16, 33, 31, 22, 54, 31, 16, 24, 14, 45])})]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(train_loader))[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbb7bf1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,\n",
       " tensor([[   2,  208, 1935,  ...,    0,    0,    0],\n",
       "         [   2,  160, 1759,  ...,    0,    0,    0],\n",
       "         [   2,   10, 1171,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [   2, 1485,   13,  ...,    0,    0,    0],\n",
       "         [   2,   57,   98,  ...,    0,    0,    0],\n",
       "         [   2,   10,  409,  ...,    0,    0,    0]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(enumerate(train_loader))[0][1]['input']),list(enumerate(train_loader))[0][1]['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2f27382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([[   2,   87, 9320,  ...,    0,    0,    0],\n",
      "        [   2,   10,   36,  ...,    0,    0,    0],\n",
      "        [   2, 5714,    1,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2,  533, 1161,  ...,    0,    0,    0],\n",
      "        [   2,    1,  160,  ...,    0,    0,    0],\n",
      "        [   2,   13,  810,  ...,    0,    0,    0]])\n",
      "tensor([[  87, 9320,    5,  ...,    0,    0,    0],\n",
      "        [  10,   36, 3839,  ...,    0,    0,    0],\n",
      "        [5714,    1,  208,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 533, 1161, 4952,  ...,    0,    0,    0],\n",
      "        [   1,  160,  385,  ...,    0,    0,    0],\n",
      "        [  13,  810,  977,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "for i, seq in list(enumerate(train_loader)):\n",
    "    print(i)\n",
    "    print(seq['input'])\n",
    "    print(seq['target'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d67ad39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   2,   87,   88,  ...,    0,    0,    0],\n",
       "         [   2, 3729,   10,  ...,    0,    0,    0],\n",
       "         [   2, 6172, 3013,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [   2, 1492, 2463,  ...,    0,    0,    0],\n",
       "         [   2, 1696,  139,  ...,    0,    0,    0],\n",
       "         [   2, 5914,   77,  ...,    0,    0,    0]]),\n",
       " tensor([ 9, 14, 34, 11, 34, 19, 10, 25, 11, 20, 19, 11, 20, 31, 12, 24, 15,  7,\n",
       "         13, 32, 17, 27, 16, 16, 17, 21, 20, 17, 31,  7, 15, 21]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract a single sequence for passing through to the model \n",
    "X_0 = list(enumerate(train_loader))[0][1]['input']\n",
    "X0_sentences_length= list(enumerate(train_loader))[0][1]['length']\n",
    "X_0, X0_sentences_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683c9450",
   "metadata": {},
   "source": [
    "# Shape tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08b5a541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM_VAE(\n",
       "  (embed): Embedding(9839, 300)\n",
       "  (encoder_lstm): LSTM(300, 256, batch_first=True)\n",
       "  (mean): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (log_variance): Linear(in_features=256, out_features=16, bias=True)\n",
       "  (init_hidden_decoder): Linear(in_features=16, out_features=256, bias=True)\n",
       "  (decoder_lstm): LSTM(300, 256, batch_first=True)\n",
       "  (output): Linear(in_features=256, out_features=9839, bias=True)\n",
       "  (log_softmax): LogSoftmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM_VAE(vocab_size = vocab_size, embed_size = embed_size,\n",
    "                 hidden_size = hidden_size, latent_size = latent_size).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9d61ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial sequence in a batch: torch.Size([32, 60])\n",
      "Hidden encoder states: torch.Size([1, 32, 256])\n",
      "After embedding: torch.Size([32, 60, 300])\n",
      "Max seq length: 60\n",
      "z shape: torch.Size([1, 32, 16])\n",
      "mu shape: torch.Size([1, 32, 16])\n",
      "log var shape: torch.Size([1, 32, 16])\n",
      "x_hat shape: torch.Size([32, 60, 9839])\n"
     ]
    }
   ],
   "source": [
    "print(f'Initial sequence in a batch: {X_0.shape}')\n",
    "hidden_encoder = model.init_hidden(batch_size) \n",
    "print(f'Hidden encoder states: {hidden_encoder[0].shape}')\n",
    "x_embed, max_pad_len = model.get_embedding(X_0)\n",
    "print(f'After embedding: {x_embed.shape}')\n",
    "print(f'Max seq length: {max_pad_len}')\n",
    "\n",
    "# Packing the input\n",
    "packed_x_embed = torch.nn.utils.rnn.pack_padded_sequence(input= x_embed, \n",
    "                                                        lengths= X0_sentences_length, \n",
    "                                                        batch_first=True, \n",
    "                                                        enforce_sorted=False)\n",
    "#print(f'after packing {packed_x_embed}')\n",
    "# Encoder\n",
    "z, mean, log_var, hidden_encoder = model.encoder(packed_x_embed, \n",
    "                                        max_pad_len, \n",
    "                                        hidden_encoder)\n",
    "print(f'z shape: {z.shape}')\n",
    "print(f'mu shape: {mean.shape}')\n",
    "print(f'log var shape: {log_var.shape}')\n",
    "# Decoder\n",
    "x_hat = model.decoder(z, packed_x_embed, max_pad_len)\n",
    "print(f'x_hat shape: {x_hat.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9a7b9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9.2340, -9.1213, -9.3122,  ..., -9.3198, -9.1470, -9.2381],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hat[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8687208",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3957aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip training\n",
    "#Loss = VAE_Loss()\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr= training_setting[\"lr\"])\n",
    "#trainer = Trainer(train_loader, test_loader, model, Loss, optimizer)\n",
    "\n",
    "#train_losses = []\n",
    "#test_losses = []\n",
    "#for epoch in range(training_setting[\"epochs\"]):\n",
    "#    print(\"Epoch: \", epoch)\n",
    "#    print(\"Training.......\")\n",
    "#    train_losses = trainer.train(train_losses, epoch, training_setting[\"batch_size\"], training_setting[\"clip\"])\n",
    "#    print(\"Testing.......\")\n",
    "#    test_losses = trainer.test(test_losses, epoch, training_setting[\"batch_size\"])\n",
    "\n",
    "#plot_elbo(train_losses, \"train\")\n",
    "#plot_elbo(test_losses, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c8d425",
   "metadata": {},
   "source": [
    "# Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8281f7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"models/LSTM_VAE.pt\", map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e14b12cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z shape: torch.Size([1, 32, 16])\n",
      "mu shape: torch.Size([1, 32, 16])\n",
      "log var shape: torch.Size([1, 32, 16])\n"
     ]
    }
   ],
   "source": [
    "# Run the trained model on a single batch\n",
    "# Encoder\n",
    "z, mean, log_var, hidden_encoder = model.encoder(packed_x_embed, \n",
    "                                        max_pad_len, \n",
    "                                        hidden_encoder)\n",
    "print(f'z shape: {z.shape}')\n",
    "print(f'mu shape: {mean.shape}')\n",
    "print(f'log var shape: {log_var.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2592fb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1077,  1.1136, -0.5670, -0.6029, -1.0720,  1.3184,  0.6331,\n",
       "          -0.1960,  0.6857,  0.8333,  0.8115,  1.0224, -0.8659, -1.1526,\n",
       "           0.5827,  0.3481],\n",
       "         [-1.2044,  1.2788, -2.3048, -0.8692, -0.8205,  0.2371, -1.8795,\n",
       "           0.7737,  0.9681, -1.2852, -0.8859,  0.4227, -1.3879, -1.8224,\n",
       "           0.3157,  0.1263],\n",
       "         [ 1.0516,  0.2249,  0.0330,  0.7451, -1.1266, -2.7080,  0.5207,\n",
       "          -1.3420,  0.2020, -0.1656, -0.5253,  0.4150,  0.3043,  1.1146,\n",
       "           0.8488, -0.3908],\n",
       "         [-1.1080,  0.0394,  1.6549, -2.1516,  0.4683,  0.1034,  0.3168,\n",
       "          -1.8934, -0.2395,  0.1188,  0.0279, -0.7378,  0.7542, -0.6265,\n",
       "          -0.0839, -0.4535],\n",
       "         [ 0.1416,  0.4957, -1.5157, -0.7472,  1.3484,  0.4683, -0.6200,\n",
       "          -0.2064,  0.0377, -0.8698, -0.7952,  0.1925,  0.1340,  0.9371,\n",
       "          -0.1409,  0.5852],\n",
       "         [-0.5278,  0.7211, -0.9456,  1.7364,  0.1067,  2.0498,  1.8641,\n",
       "          -0.1112, -0.6168, -0.3236,  1.0144, -0.3809, -0.0384,  0.4708,\n",
       "          -0.8848,  2.3858],\n",
       "         [ 1.6706, -2.1359, -1.5357,  0.0989, -0.6197, -1.9512, -0.3462,\n",
       "           0.7091,  1.1559,  0.9150,  0.6836, -0.3179, -0.0841,  0.2348,\n",
       "           1.1762,  2.2349],\n",
       "         [-0.4842,  0.3309, -1.9690,  0.2528,  1.3688,  1.0563, -0.4703,\n",
       "          -0.1242, -0.6386,  0.5990,  0.7571, -0.6717,  0.3942,  0.8704,\n",
       "           1.0974, -1.0912],\n",
       "         [-1.0257,  0.8422,  0.6568,  1.4458, -0.4126,  0.1832,  0.6807,\n",
       "          -1.0709, -0.6128,  1.7222, -1.0158, -0.4314,  1.1806,  0.2743,\n",
       "          -1.0169, -0.0481],\n",
       "         [-0.2793,  0.1808,  1.0833, -0.4488,  1.0763,  0.4731, -2.4595,\n",
       "          -0.3168,  0.7957,  1.6025,  0.1605, -2.5476,  1.7111, -0.1205,\n",
       "          -0.9633, -1.3729],\n",
       "         [ 1.0127,  0.8800, -0.7754,  0.7615, -0.8356, -0.3693,  0.1049,\n",
       "          -0.7757,  0.7598,  0.8229, -0.3543, -0.7274, -1.3925, -0.0666,\n",
       "          -0.7433, -0.9288],\n",
       "         [ 1.6913, -1.7211,  0.5595, -0.6668, -0.9836,  1.4500, -0.2021,\n",
       "          -0.3564,  0.4745,  0.5006, -2.1961, -1.6624, -2.2374, -0.0659,\n",
       "          -2.6724,  2.1480],\n",
       "         [-1.7422, -1.8705, -0.2465, -1.0262,  0.6638, -0.0436,  0.4776,\n",
       "          -1.9684, -1.0597, -1.4309, -0.5712,  1.3537, -0.1020, -0.5168,\n",
       "          -1.4548, -1.1230],\n",
       "         [-1.4272, -2.3250, -0.5219,  0.3688, -1.9181, -0.6753,  0.6467,\n",
       "           0.6890, -1.1205,  1.2910, -0.0766,  1.1204,  0.8405, -0.7196,\n",
       "          -0.9936,  1.2470],\n",
       "         [ 0.9393, -0.0922, -0.4674,  0.4828,  0.0373, -0.4931, -1.7808,\n",
       "           0.6121, -0.0253,  0.9654, -0.3250,  1.0547,  1.1700,  1.0001,\n",
       "          -1.1461, -0.6058],\n",
       "         [-1.1786, -1.1267, -0.5679, -0.7063,  1.1006,  1.2303, -0.0648,\n",
       "           0.0057, -2.1123,  0.1528, -0.0644,  1.5485,  0.4560, -0.4989,\n",
       "          -0.8291, -0.3999],\n",
       "         [-0.5838, -0.4743, -0.9754,  0.3409, -1.4337, -0.1220,  0.0597,\n",
       "           0.7167,  0.8276, -0.6459, -1.7727,  0.8362, -0.5534,  0.4429,\n",
       "          -1.3683,  0.4961],\n",
       "         [-0.6890,  0.3907,  0.3053,  1.1724, -1.0728,  0.1222,  0.8177,\n",
       "           0.7065, -1.0412,  0.9982,  0.6279,  1.0123,  1.5283,  1.7829,\n",
       "           0.7124,  0.5004],\n",
       "         [-0.1293, -0.5618,  2.0342, -1.4324,  1.5620, -0.6080,  0.7869,\n",
       "           1.9115,  0.3390,  0.2143,  1.1009,  0.4670, -1.3890,  1.3724,\n",
       "           0.1731, -0.4320],\n",
       "         [-1.1525, -0.5741, -1.4342, -0.2281,  0.6130, -0.1122,  0.5052,\n",
       "          -0.0994, -1.5841, -1.2161,  1.6743, -1.1117, -0.2887, -0.1834,\n",
       "           1.0704, -0.5541],\n",
       "         [ 1.8435,  1.4009, -0.2256, -0.1106,  0.9817,  0.2346, -1.0323,\n",
       "           0.4069, -0.6073,  0.4941, -0.4217, -2.5868, -1.3546,  0.0554,\n",
       "          -1.7248, -0.2289],\n",
       "         [ 0.4241,  0.5356, -0.5263,  0.3134,  0.5203,  0.7423, -0.7168,\n",
       "           2.1149, -1.6087, -0.5697,  0.5401, -0.3422, -0.9907, -0.7143,\n",
       "           1.6204, -0.5586],\n",
       "         [-0.4030,  0.5482, -0.2953, -0.0793, -0.8207,  1.1625, -1.0382,\n",
       "          -1.1516,  1.5358, -0.8131, -0.1989,  0.3889, -0.0411,  0.3637,\n",
       "           0.3080,  0.9186],\n",
       "         [-1.2624, -0.7342,  0.4457, -1.4463, -0.0639, -0.7849, -1.5847,\n",
       "          -0.9456, -0.6858, -0.1338, -0.9322,  0.6946, -1.3592, -0.0932,\n",
       "          -0.9250, -0.1328],\n",
       "         [ 0.5549, -1.1453,  0.4192, -2.0393,  1.3874, -0.2684,  0.6575,\n",
       "          -0.4050,  0.1451, -0.2745,  1.3235,  0.5846,  1.1884,  2.0716,\n",
       "          -0.4610,  0.9459],\n",
       "         [-0.0605, -0.8328, -0.3014, -0.6212, -0.8536, -0.1848,  0.9044,\n",
       "          -1.2085,  2.2556,  0.1254, -0.0103,  0.0467,  1.0649,  0.4907,\n",
       "          -0.0260, -0.7790],\n",
       "         [-0.2135, -0.3572, -0.9167, -0.5293, -0.8244, -2.1921, -0.1492,\n",
       "           1.1636,  1.1839,  0.7313, -0.0772, -1.0865,  0.7710,  1.4701,\n",
       "          -1.4476, -0.6061],\n",
       "         [ 0.0626, -1.6532, -0.8939, -0.9519, -0.4603, -0.2512,  0.3351,\n",
       "           0.9488,  0.8695, -0.7414, -1.8360,  0.1463, -0.2408, -0.3869,\n",
       "          -1.5504, -0.9407],\n",
       "         [-0.4646,  2.8907,  0.5486, -2.0538,  0.9035, -0.6775, -0.0667,\n",
       "           0.9042,  1.1954,  1.0914, -0.4732,  1.9197,  0.6505,  0.9391,\n",
       "          -0.7749,  0.6171],\n",
       "         [ 1.6476, -0.1626, -0.0784, -1.5160, -1.0082,  0.7371, -0.9309,\n",
       "          -0.9347,  0.4540,  0.3632,  1.4771, -1.6896, -0.0090,  0.8380,\n",
       "           0.4403,  0.0381],\n",
       "         [ 1.9260,  2.3343, -1.5500,  0.4609,  0.7459,  0.3324, -0.7602,\n",
       "           1.9243,  0.2967, -1.4373,  1.3526, -1.6319, -0.8945,  1.7484,\n",
       "          -0.2981, -0.6966],\n",
       "         [-0.7905, -1.5937,  2.1332,  1.3888, -0.5522,  1.2708, -0.3259,\n",
       "           2.1416,  0.7986,  1.8795, -1.3574,  0.4482,  1.0899,  0.3456,\n",
       "           0.4109, -0.8503]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "58d7c876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_hat shape: torch.Size([32, 60, 9839])\n"
     ]
    }
   ],
   "source": [
    "# Decoder\n",
    "x_hat = model.decoder(z, packed_x_embed, max_pad_len)\n",
    "print(f'x_hat shape: {x_hat.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b07b224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -8.8279,  -5.9137, -19.1463,  ..., -15.9236, -16.8142, -11.2007],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hat[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82a9b7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 16])\n",
      "the company said it will close the <unk> offering for its $ n million acquisition of first union corp . which has relied heavily on\n",
      "the <unk> <unk> <unk> is a <unk> and <unk> <unk> <eos> who <unk> them <eos> <eos> <eos> and they are on their business <eos> <eos>\n"
     ]
    }
   ],
   "source": [
    "z1 = torch.randn(1,1,latent_size).to(device)\n",
    "z2 = torch.randn(1,1,latent_size).to(device)\n",
    "print(z1.shape)\n",
    "sos = \"<sos>\"\n",
    "unk = \"<unk>\"\n",
    "sample1 = model.inference(25 , sos, z1)\n",
    "sample2 = model.inference(25 , sos, z2)\n",
    "\n",
    "print(sample1)\n",
    "print(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e92ac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate(model, n_interpolations, sos, sequence_length):\n",
    "  # # Get input.\n",
    "  text1 = model.inference(sequence_length , sos, z1)\n",
    "  text2 = model.inference(sequence_length , sos, z2)\n",
    "  alpha_s = torch.linspace(0,1,n_interpolations)\n",
    "  interpolations = torch.stack([alpha*z1 + (1-alpha)*z2  for alpha in alpha_s])\n",
    "  samples = [model.inference(sequence_length , sos, z) for z in interpolations]\n",
    "  return samples, text1, text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "45a228eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence: officials holders enfield chief of the american association of operations\n",
      "Second sentence: <pad> a leading employee presidents and the carrier has moved\n",
      "<pad> a leading employee presidents and the carrier has moved\n",
      "<pad> a leading employee presidents and the carrier has moved\n",
      "<pad> a leading employee presidents and the carrier has moved\n",
      "<pad> a leading employee presidents and the carrier has moved\n",
      "<pad> a leading employee presidents of the commission a spokesman\n",
      "<pad> a <unk> assembly plant at <unk> <unk> who had\n",
      "<pad> a <unk> assembly plant at <unk> <unk> who had\n",
      "<pad> a <unk> spokesman said <eos> <unk> <unk> inc .\n",
      "<pad> a <unk> spokesman said <eos> <unk> <unk> inc .\n",
      "officials holders enfield chief of the chain quite quite <unk>\n",
      "officials holders enfield chief of the chain quite quite <unk>\n",
      "officials holders enfield chief of the chain quite quite <unk>\n",
      "officials holders enfield chief of the american association of operations\n",
      "officials holders enfield chief of the american association of operations\n",
      "officials holders enfield chief of the american association of operations\n",
      "officials holders enfield chief of the parent company which holds\n",
      "officials holders enfield chief of the american association of operations\n",
      "officials holders enfield chief of the american association of operations\n",
      "officials holders enfield chief of the american association of operations\n",
      "officials holders enfield chief of the american association of operations\n"
     ]
    }
   ],
   "source": [
    "samples, text1, text2 = interpolate(model, 20, \"company\",  10)\n",
    "print(\"First sentence:\", text1)\n",
    "print(\"Second sentence:\", text2)\n",
    "\n",
    "for sample in samples: print(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
